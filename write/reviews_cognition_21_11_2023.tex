\section{Reviewer #1}

In this paper the authors examine how switching
between categories during learning affects acquisition of
category knowledge, and focuses on the roles played by type
of category structure and motor responses. Overall the
article is well done and makes a significant novel
contribution to the field.

I have one major concern and a few minor ones.

\subsection{Comment}
My main concern is that all of the presentation of results
is collapsed across both categories. That is, we see how
well subjects are learning overall, but we have no idea how
well they are learning each individual category. For
example, Figure 3 shows accuracy across both tasks, but not
for individual tasks. Figure 5 shows switch costs from
"same" to different tasks so it is hard to assess whether
the high swich cost from task 1 to task 2 is actually a
switch cost, or whether it is just that subjects are worse
at task 2 than task 1. I think it is important to assess
whether subjects are indeed learning both category
structures simultaneously, or whether they are ignoring one
category in order to focus on getting a high score on the
other category. Both of these strategies could lead to the
level of accuracy found by the authors.

\subsection{Response}
Make some figures / do some analyses to assess this. Also
seek to rephrase / frame results to properly emphasise
whether or not this is important.

Minor concerns:
It is unclear how to interpret A in Figure 4. I understand
it is a measure of "initial learning" from the model, but it
isn't clear to the reader which trials / phase of learning
really is weighted in the A measure.

- Perhaps examine the description of the learning curve
  model to look for ways to frame this more clearly

Fig 8: The red and orange lines overlap too much and can't
tell there are two lines there. Maybe make them have larger
symbols and/or dotted/patterned lines?

- Play around with this to see if it helps

Some of the switches were full reversals, such as between
two II tasks and the two 2DRB tasks (page 5). It might be
worthwhile to discuss this choice further, as reversal
learning has been shown to have some qualitatively different
characteristics in comparison with transfer to novel
effectors.

- I'm not sure what to do with this comment since no
  references were provided. One possibility is to simply add
  a section to the discussion section that addresses the
  concept of a full reversal. If nothing else this can be
  done from the perspective of category learning (e.g.,
  2-stage model).

All of the categories used the same stimulus space and hence
there was perceptual overlap in the critical features (even
though the two different categories were in different
colors). Might this have led to high levels of response
conflict and might the results overall have been different
without this overlap? Other switching studies such as Turner
et al 2017 have used non-overlapping categories - in Turner
et al the bar widths were across different ranges in the two
categories. This should also be discussed.

- Sure. We can add this to the dicussion. A key point will
  be that Turner et al also used unique response keys, so the
  present study implies that perceptual overlap is no big
  deal if given unique motor plans.

\section{Reviewer #2} 
\subsection{Comment}
This paper describes an interesting investigation into
switch costs during the learning of new categorization
tasks. The authors systematically manipulate the
dimensionality of the category rules, their likely reliance
on declarative vs. procedural memory systems, and whether
they are associated with overlapping or unique response
sets. The data are analyzed in terms of learning curves and
decision bound models, and a new task switching model is
used to simulate some aspects of the results. The most
consequential factor for learning and switch costs appears
to be whether responses are overlapping or task-unique. This
is a well-written paper addressing an interesting topic -
the interaction of task/category learning and task
switching. The basic task condition design and the analyses
are sophisticated, but a number of serious drawbacks make it
very difficult to draw any clear conclusions from these
data, in my view. I hope my comments will aid the authors in
strengthening the paper.

\subsection{Response}
I'm not 100\% sure what to do with this. The main effect in
our study is that unique motor plans are key and that
result really could not be any clearer. It is possible that
all the other features of the study distract from this main
point. For now plan to examien the paper to look for ways to
better emphasise the main point.

\subsection{comment}
1- There are a number of issues that render the results of
this study hard to interpret. One very important one is
statistical power. The authors supply no sample size
justification, but a between-subjects design with only 10-16
subjects per cell (and not a ton of trials) seems to run a
large risk of being underpowered. The authors' finding of
non-significant differences in spite of a supposedly large
effects size for one of the interactions corroborates this
concern, and the error bars are generally very large here.
There are many null effects reported, and it is really hard
to know how to interpret those. Many of them could plausibly
simply be a reflection of a substantially underpowered
study.

\subsection{Response}
Well, this may well be true, but the reviewer seems to
overstate the issue. There are only a couple results with
high effect size and n.s. so it seems likely power was only
a small issue. I don't see anything to do about this given
how ancient the data is, and given the writing already
addresses power. I suppose, if resources permit, I could run
a bunch of data online but this is time demanding and I
don't have the bandwidth at the moment.

\subsection{comment}
2- The effects of the "motor plan factor" could have
simpler, alternative interpretations than the ones favored
by the authors. For instance, the fact that accuracy rates
are higher in early learning for the overlapping response
sets may simply reflect that here the chance of guessing the
correct response is greater than with 4 response options.
Relatedly, it seems unsurprising that people would
ultimately learn better in the non-overlapping response
conditions, because here correct performance feedback
supplies more information than it does for the overlapping
2-response conditions.

\subsection{Response}
The guessing thing about intial accuracy seems like the
explanantion we ourselves favour. Furthermore this seems
like an odd thing for the reviewer to be focusing on. It is
not one of our major results let alone the major result of
the paper. All the same, we should examine our writing to
see how this reviewer may have gotten hung up on this
particularity and revise where we think it will help.

\subsection{comment}
3- Given that most participants did not actually learn the
correct category rules for the majority of the tasks, it
seems difficult to interpret the switch costs between these
tasks. Put another way, if participants have not formed
proper task sets (they are mostly guessing based on rather
imperfect heuristics), how can we attribute switch costs to
specific task parameter components (like the supposed memory
systems involved, etc.)?

\subsection{Response}
First, I can see how indeed it would be difficult to
interpret switch costs given failure to learn the initial
tasks. I think this should probably appear in the discussion
if it isn't already there. Second, the reviewer seems to
overstate the guessing component. Participants use 1D rules
where such rules are suboptimal, but that is different from
guessing. The failure to learn ``proper task sets'' is in
some cases precisely the point. The main finding isn't
anything about switch cost as a function of attention or
memory condition etc. It is about motor plans and it is
difficult to imagine an effect than is clearer than that in
our data.  Examine our writing to see if we can be clearer
in our emphasis or structure etc to make this point and
avoid this confusion.

\subsection{comment}
4- The to-be-learned tasks have different difficulty levels,
which will lead to well-known "asymmetrical switch costs"
(briefly mentioned in the discussion). This complicates
interpretation of the switch costs between the tasks,
especially since there are no single-task baseline
conditions to determine the difficulty of each task in
isolation.

\subsection{Response}
There may be something important to check here. In
particular, a switch cost may be ill-defined when the tasks
aren't learned in the first place... though I am unsure how
this would be truly undermining of our results. If a task is
harder and then people should take longer to response unless
they have given up... Anyway, not so clear to me at the
moment how big a deal this is. Keep an eye on it as it
evolves in our response to previous and subsequent comments.

\subsection{comment}
5- The authors describe several models of cognitive control
(Botvinick et al., 2001; Blais et al., 2007; Verguts &
Notebeart, 2008) as models of task switching, but none of
these models actually incorporate any explicit mechanism for
changing task sets. They only have mechanisms for regulating
top-down control within a task set (based on conflict).
Better references for related models that do entail task
switching components would be Gilbert & Shallice (2002, Cogn
Psychol) and Brown, Reynolds & Braver (2006, Cogn Psychol).

\subsection{Response}
Check and digest these references.

\subsection{comment}
6- Speaking of models, some of the details, as well as the
general utility of the modeling exercise in the current
paper was not entirely clear to me. The model seems like a
post-hoc attempt at capturing one specific aspect of the
data pattern in the current data set. Given that there was
only a tiny model space considered (two rivals), and the
parameter values seem to be handcrafted, I don't think much
can be drawn from this. The supposed novelty of mutually
inhibitory response units is also in fact part of previous
models. For instance, Botvinick et al.'s (2001) conflict
monitoring model has mutually inhibitory response units.

\subsection{Response}
Check the claim about Botvinick et al.'s (2001) model. I
suspect there is confusion here that we should explicitly
address in our writing. We can also do something like a
sensitivity analysis to show that the two different
architectures reliably predict what they predict in a
qualitative way.

\subsection{comment}
7- Finally, there is some additional relevant literature the
authors may want to consider in framing the current work.
First, Anne Collins and Michael Frank did a series of
studies on people learning hierarchical task rules from
scratch that (if acquired) would eventually produce switch
costs (e.g., Collins & Frank, 2013, Psych Rev; 2016,
Cognition; 2016, PLoS Comp Biol). Second, Flesch and
Summerfield have some recent work that involves learning
categorical rules under single-task or interleaved regimes,
and subsequent assessment of switch costs (among other
things) (e.g., Flesch et al., 2018, PNAS; Neuron, 2022,
etc.).

\subsection{response}
Check and digest the cited literature.

\section{Reviewer #3} 
The manuscript explores interactions between category
learning and task switching. In the experiment, participants
learned two categorization tasks that were interleaved
across trials. Different groups learned different pairs of
categorizations of oriented ellipses, which were based
either on ellipse length alone ("1D rule based") or a
combination of ellipse length and orientation according to a
rule ("2D rule based") or integration of the two dimensions
("information integration"). The different categorizations
were cued by ellipse color and could be associated with the
same or different pair of response keys (also varied across
groups). Results are presented for categorization accuracy
(raw scores and latent variables from a learning curve fit),
effects of task switching, decision bound model fits and a
computational model.  Separating response keys helped
learning when both tasks were 2D. Accuracy tended to be
lower when switching between categorization tasks across
trials rather than repeating them. Participants seemed to
apply a simple 1D categorization rule for both tasks except
in the case where both tasks used 2D rules and different
response keys.

The research question is an interesting one and is
relatively under-researched (although maybe not as
under-researched as the authors suggest). The results
suggest the approach has promise, but as currently analyzed
do not present a clear enough picture to warrant
publication. As the authors acknowledge, their design seems
under-powered to detect effects that are emerging in the
results. Aspects of the results undermine the aim of the
design and some of the conclusions drawn. The computational
model isn't convincing. The manuscript is also too tersely
written, with inconsistent terminology, for the reader to
easily understand a complex experimental design and set of
results. These concerns and other more minor issues are
explained below.

Main concerns

\subsection{comment}
I don't think the results are clear enough to warrant
publication. I didn't come away with a clear sense of what
the authors are claiming to show about categorization, task
switching or their intersection. The results seem suggestive
rather than leading to clear conclusions. This
inconclusiveness seems to stem from several factors:

\subsection{response}
Hard to see where this reviewer is coming from with this.
Our main result seems overwhelmingly clear. Perhaps the
difficulty is in having so many design factors so it's too
easy for the reader to get a bit lost. We should seek ways
to revise the writing to better direct the readers attention
and understanding.

\subsection{comment}
- The introduction does not identify specific hypotheses or
predictions about what patterns of results should be
observed, and how these predictions relate to existing
theories of categorization and task switching.

\subsection{response}
Our original point of view was that no existing theory makes
any predictions of any kind... although I suppose we add
some nuance to this in our discussion. Perhaps there is
something in what we have written there that can be helpful.
Another possibility is that some of the literature previous
reviewers have asked us to include may indeed make some
predictions. A final possibility is to sketch predictions
from some arm chair models. In any case, there is probably
room to revise a bit to accomodate this comment.

\subsection{comment}
- The designs are relatively underpowered with N≤16 in each
group and a high degree of variability across participants
in how they learned to categorize (Figure 7).

\subsection{response}
Fair enough but hard to cope with this. Perhaps collapse
across conditions or do a better job of describing the
collapsing across conditions. The only other possibility I
see is to collect more data, which would be ideal. Work out
how much this would cost on prolific etc.

\subsection{comment}
- A main aim of the study is to investigate effects of
switching between different memory systems for different
categorizations, but the results in Figure 7 suggest that
only a small subset of participants learned different
categorization strategies for the two tasks. Participants
commonly over-extended a 1D rule to the paired 2D/II task,
or applied the same 2D (GLC) strategy to both. This somewhat
undermines the idea that participants are task switching,
and seriously undermines analyses of within/between system
switching comparisons.

\subsection{response}
I don't think this was a main aim so we can reframe the
writing to better scope our results. On the other hand, the
failure of many to switch is somewhat the point. That is, it
is meaningul signal itself. Look for ways to better
communicate this in our writing.

\subsection{comment}
- Many analyses seem to average across the two subtasks
within a condition, which doesn't seem like the correct
approach when theoretically these subtasks are very
different.

\subsection{response}
Consider an analysis approach that keeps sub-tasks apart.

\subsection{comment}
- The experiments lack a potentially important control
condition. If the effects are being linked to task switching
rather than the general requirement of learning two
different categorizations, shouldn't there be matched
conditions in which learning is blocked rather than
interleaved?

\subsection{response}
Blocked learning will have it's own form of intereference so
it isn't clear to me how this would be a control condition.
We should think about the utility of including a single-task
control condition, which may also appease earlier
reviewerers.... but think about the logic here. What is
their main concern?

\subsection{comment}
- The experiment has a complex design with 10 separate
groups. The writing aims to convey the design concisely,
but there are places where more explanation seems necessary
for the reader to easily follow the text.

\subsection{response}
Look for opportunities to write about the design more
clearly.

\subsection{comment}
- It is a puzzling choice to implement effects of cue-level
cognitive control as changes in connection strength (p.7),
rather than reduced activity as inhibition is usually
simulated. Inhibition via reduced activity is implemented
for "cognitive inhibition" of response options in Equation
2. Why the difference?

\subsection{response}
I don't follow this comment. Keep an eye on it as we re-read
our paper.

\subsection{comment}
- I couldn't figure out from the descriptions and Figure 2
how cognitive inhibition is applied in the "same motor plan"
conditions, where the same responses (motor units) are used
for both subtasks. As described above, I wasn't entirely
sure from the description whether "response option" in
Equation 2 relates to the Category Label or Motor Unit layer
shown in Figure 2, but think it must be the latter based on
the statement that "evidence for a particular response
option … is inhibited in proportion to both the cue-level
and motor-level cognitive control terms", and that Equation
2 includes lateral inhibition from other responses. But the
same set of responses (motor units) are required for both
tasks in "same motor plan" conditions, so cognitive
inhibition can't be separately applied to "responses
associated with the active and inactive tasks" because these
are the same responses in both cases.

\subsection{response}
Seems like we can revise to clear this up.

\subsection{comment}
The authors may have overlooked relevant previous research
studying task learning and task switching.

- Work by Anne Collins and Michael Frank seems directly
relevant (e.g. Collins & Frank, 2013 Psychological Review;
Collins et al., 2014 Journal of Neuroscience).

\subsection{response}
Examine and digest this literature.

\subsection{comment}
- Also relevant is a line of research studying switching
between instructed versus trial-and-error learned categories
(e.g. Forrest et al., 2014 JEP:LMC; Dreisbach & Haider,
2008, Psychological Research), suggesting that the latter
design can lead participants to memorize undifferentiated
sets of stimulus-response mappings rather than structured
task sets. I particularly had this line of work in mind
given the reported finding of limited learning benefits (and
task switching costs) when comparing "within system"
pairings to "between system" pairings. RB-RB and II-II
pairings involve the same categorization on all trials, just
changing assignment to response keys, so I'd expect these
pairings to be much more easily learned if participants
extracted higher-level structure in the task.

\subsection{response}
Examine and digest the cited literature.

\subsection{comment}
- See also work by Musslick and Cohen (e.g. 2021, Trends in
Cognitive Sciences).

\subsection{response}
Examine and digest the cited literature.

\subsection{comment}
Minor comments

p.3. "are described in Table ." Missing number. Same on p.9.

p.4, Table 1. What does "cr" stand for?

p.6. For subtasks using different response keys but the same
categorization task (cj#cr and ii#cr conditions) it is
stated that "response assignments were reversed", but is
this relative to spatial position or finger used?

p.7. "we computed the proportion of participants whose
responses were best fit by each type of model". Please
clarify how this was achieved. Was it a simple count of how
many responses would be correctly categorized assuming the
rule, or a parametric error-minimization test? Did the test
of fit adjust for the number of free parameters in each
model?

p.7. "The model is illustrated in Figure 2 for an
application to the 1D RB conditions in which stimuli S1 -
S27 belong to category A in the first subtask and category C
in the second subtask, and stimuli S28 - S56 belong to
categories B and D in subtasks 1 and 2, respectively." This
description implies there's a condition with two 1D RB
subtasks, but I this isn't one of the condition listed in
Table 1.

p.9. Is task switching accuracy calculated across all
learning trials? It seems likely there would be different
patterns early versus late in learning. Are similar results
seen if analysis is restricted to trials late in learning,
as is done for the decision bound model analysis?

p.10. In Figure 5, Left and corresponding analyses, why
divide trials into separate 2|1 and 1|2 switches, given that
the assignment of subtask to 1 and 2 is presumably
arbitrary? For Figure 5, Right, Stay trial accuracy should
be calculated separately for RB and II, because the observed
differences between conditions could reflect this difference
rather than stay/switch.

p.10. How quickly did participants in the "unique motor
plans" conditions learn to map ellipse colors to their two
hands? This learning seems a necessary or least useful basis
for then learning the categorization.

p.14, Figure 7. Please explain in the caption what the
different colors correspond to in the category space
(different responses/actions?).

p.14. "This table shows that only the cj4cr condition had a
significantly greater than chance number of participants who
were best fit by an optimal model on both subtasks". I don't
know how to relate this statement to the plot in Figure 7
(row 4, columns 3 and 4) which seems to indicate that GLC
models were mostly the best fits. I interpreted the earlier
discussion to mean that GCC models were optimal for the
2D-RB categorization.

p.17. "unique motor plans led to lower initial accuracy
regardless of attention and memory system demands, perhaps
reflecting difficulty in remembering which motor plans are
correctly associated with each context". I'd interpreted
this difference to be a simple consequence of the fact that
initial chance performance would average 0.25 when there are
4 possible response keys and 0.5 when there were 2 response
keys.
